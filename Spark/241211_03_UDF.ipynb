{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d2a918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 16:08:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-110.ap-northeast-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>241211_03_UDF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4967d8e670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"241211_03_UDF\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615262fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    (\"A\", \"2022-04-16\", 31200),\n",
    "    (\"B\", \"2022-04-17\", 41200),\n",
    "    (\"C\", \"2022-04-11\", 31500),\n",
    "    (\"D\", \"2022-04-12\", 21500),\n",
    "    (\"E\", \"2022-04-13\", 51000)\n",
    "]\n",
    "columns = [\"product\", \"date\", \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37e8d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|product|      date|price|\n",
      "+-------+----------+-----+\n",
      "|      A|2022-04-16|31200|\n",
      "|      B|2022-04-17|41200|\n",
      "|      C|2022-04-11|31500|\n",
      "|      D|2022-04-12|21500|\n",
      "|      E|2022-04-13|51000|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data =datas, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f0d2e",
   "metadata": {},
   "source": [
    "# UDF \n",
    "\n",
    "user defined function:\n",
    "query 에서 사용하는 사용자 정의 함수\n",
    "\n",
    "1. 파이썬의 함수로 정의한다\n",
    "2. spark.udf.register()로 등록한다\n",
    "3. sql문 안에서 func처럼 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d20dedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 16:18:34 WARN SimpleFunctionRegistry: The function squared replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(n)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "#파이썬 함수 만들기\n",
    "def squared(n):\n",
    "    return n*n\n",
    "\n",
    "#함수 등록\n",
    "spark.udf.register('squared', squared, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff9e37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd91b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|product|      date|price|\n",
      "+-------+----------+-----+\n",
      "|      A|2022-04-16|31200|\n",
      "|      B|2022-04-17|41200|\n",
      "|      C|2022-04-11|31500|\n",
      "|      D|2022-04-12|21500|\n",
      "|      E|2022-04-13|51000|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM product').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "031030af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|price|squared(price)|\n",
      "+-----+--------------+\n",
      "|31200|     973440000|\n",
      "|41200|    1697440000|\n",
      "|31500|     992250000|\n",
      "|21500|     462250000|\n",
      "|51000|    2601000000|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT price, squared(price) FROM product').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf2fb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_number(n):\n",
    "    units = [\"\",\"십\",\"백\",\"천\",\"만\",]\n",
    "    nums = '일이삼사오육칠팔구'\n",
    "    result = []\n",
    "    i=0\n",
    "    while n > 0:\n",
    "        n,r = divmod(n,10)\n",
    "        if r > 0:\n",
    "            result.append(nums[r-1]+units[i])\n",
    "        i += 1\n",
    "    return \"\".join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4625d05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일백이십삼'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_number(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "707cb889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.read_number(n)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"read_number\", read_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "396b6ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|price|read_number(price)|\n",
      "+-----+------------------+\n",
      "|31200|      삼만일천이백|\n",
      "|41200|      사만일천이백|\n",
      "|31500|      삼만일천오백|\n",
      "|21500|      이만일천오백|\n",
      "|51000|          오만일천|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select price, read_number(price) from product').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43ad5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['price, unresolvedalias('read_number('price), None)]\n",
      "+- 'UnresolvedRelation [product], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "price: bigint, read_number(price): string\n",
      "Project [price#21L, read_number(price#21L) AS read_number(price)#296]\n",
      "+- SubqueryAlias product\n",
      "   +- LogicalRDD [product#19, date#20, price#21L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [price#21L, pythonUDF0#299 AS read_number(price)#296]\n",
      "+- BatchEvalPython [read_number(price#21L)], [pythonUDF0#299]\n",
      "   +- Project [price#21L]\n",
      "      +- LogicalRDD [product#19, date#20, price#21L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [price#21L, pythonUDF0#299 AS read_number(price)#296]\n",
      "+- BatchEvalPython [read_number(price#21L)], [pythonUDF0#299]\n",
      "   +- *(1) Project [price#21L]\n",
      "      +- *(1) Scan ExistingRDD[product#19,date#20,price#21L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select price, read_number(price) from product').explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9b91d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_start",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
