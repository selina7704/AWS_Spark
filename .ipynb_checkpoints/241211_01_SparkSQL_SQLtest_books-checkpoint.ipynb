{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dde1e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 10:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-110.ap-northeast-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>241211_01_SparkSQL_SQLtest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff654b226a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"241211_01_SparkSQL_SQLtest\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b918017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "user_data = [\n",
    "    Row(user_id=1, username='A', address='서울'),\n",
    "    Row(user_id=2, username='B', address='대전'),\n",
    "    Row(user_id=3, username='C', address='경기도'),\n",
    "    Row(user_id=4, username='D', address=None),\n",
    "    Row(user_id=5, username='E', address=None),\n",
    "    Row(user_id=6, username='F', address='서울'),\n",
    "    Row(user_id=7, username='G', address='경기도'),\n",
    "    Row(user_id=8, username='H', address='대구'),\n",
    "    Row(user_id=9, username='I', address='부산'),\n",
    "    Row(user_id=10, username='J', address='전주'),\n",
    "    Row(user_id=11, username='K', address='광주')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f7930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.createDataFrame(user_data)\n",
    "user_df.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aea00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = [\n",
    "    Row(book_id=1, title=\"Book A\", author_fname=\"John\", author_lname=\"Doe\", pages=300, released_year=2005, stock_quantity=55),\n",
    "    Row(book_id=2, title=\"Book B\", author_fname=\"Jane\", author_lname=\"Smith\", pages=250, released_year=2010, stock_quantity=40),\n",
    "    Row(book_id=3, title=\"Book C\", author_fname=\"Emily\", author_lname=\"Jones\", pages=180, released_year=2015, stock_quantity=20),\n",
    "    Row(book_id=4, title=\"Book D\", author_fname=\"Chris\", author_lname=\"Brown\", pages=320, released_year=2012, stock_quantity=75),\n",
    "    Row(book_id=5, title=\"Book E\", author_fname=\"Anna\", author_lname=\"Davis\", pages=270, released_year=2008, stock_quantity=35)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f10449",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = spark.createDataFrame(books_data)\n",
    "books_df.createOrReplaceTempView('books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5260a277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|user_id|username|address|\n",
      "+-------+--------+-------+\n",
      "|      1|       A|   서울|\n",
      "|      2|       B|   대전|\n",
      "|      3|       C| 경기도|\n",
      "|      4|       D|   null|\n",
      "|      5|       E|   null|\n",
      "|      6|       F|   서울|\n",
      "|      7|       G| 경기도|\n",
      "|      8|       H|   대구|\n",
      "|      9|       I|   부산|\n",
      "|     10|       J|   전주|\n",
      "|     11|       K|   광주|\n",
      "+-------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_users = '''\n",
    "SELECT * \n",
    "FROM users;\n",
    "'''\n",
    "spark.sql(query_users).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013908fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|username| address|\n",
      "+--------+--------+\n",
      "|       A|    서울|\n",
      "|       B|    대전|\n",
      "|       C|  경기도|\n",
      "|       D|주소없음|\n",
      "|       E|주소없음|\n",
      "|       F|    서울|\n",
      "|       G|  경기도|\n",
      "|       H|    대구|\n",
      "|       I|    부산|\n",
      "|       J|    전주|\n",
      "|       K|    광주|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_users2 = '''\n",
    "SELECT username,\n",
    "    IF (address IS NULL, '주소없음', address) as address\n",
    "FROM users;\n",
    "'''\n",
    "spark.sql(query_users2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18eeaac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|address|region|\n",
      "+-------+------+\n",
      "|   서울|수도권|\n",
      "|   대전|  지방|\n",
      "| 경기도|수도권|\n",
      "|   null|  지방|\n",
      "|   null|  지방|\n",
      "|   서울|수도권|\n",
      "| 경기도|수도권|\n",
      "|   대구|  지방|\n",
      "|   부산|  지방|\n",
      "|   전주|  지방|\n",
      "|   광주|  지방|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_users3 = '''\n",
    "SELECT address, \n",
    "    IF(address IN ('경기도', '서울'), '수도권', '지방') AS region \n",
    "FROM users;\n",
    "'''\n",
    "spark.sql(query_users3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a24d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|stock_quantity|quantity_level|\n",
      "+--------------+--------------+\n",
      "|            55|     재고 많음|\n",
      "|            40|     재고 중간|\n",
      "|            20|     재고 없음|\n",
      "|            75|     재고 많음|\n",
      "|            35|     재고 중간|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# books table\n",
    "# stock_quantity > 50 '재고 많음', >= 30 '재고 중간', '재고 없음'\n",
    "\n",
    "books_sql = '''\n",
    "SELECT stock_quantity, \n",
    "       IF(stock_quantity >= 50, '재고 많음', \n",
    "          IF(stock_quantity >= 30, '재고 중간', '재고 없음')) AS quantity_level\n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(books_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "975366c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|stock_quantity|quantity_level|\n",
      "+--------------+--------------+\n",
      "|            55|     재고 많음|\n",
      "|            40|     재고 중간|\n",
      "|            20|     재고 부족|\n",
      "|            75|     재고 많음|\n",
      "|            35|     재고 중간|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_1= '''\n",
    "SELECT stock_quantity, \n",
    "   CASE \n",
    "       WHEN stock_quantity >= 50 THEN '재고 많음'\n",
    "       WHEN stock_quantity >= 30 THEN '재고 중간'\n",
    "       ELSE '재고 부족'\n",
    "   END AS quantity_level\n",
    "FROM books;\n",
    "'''\n",
    "spark.sql(books_sql_1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267016f",
   "metadata": {},
   "source": [
    "# 실행 계획 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1bbe89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [stock_quantity#21L, if ((stock_quantity#21L >= 50)) 재고 많음 else if ((stock_quantity#21L >= 30)) 재고 중간 else 재고 없음 AS quantity_level#147]\n",
      "+- *(1) Scan ExistingRDD[book_id#15L,title#16,author_fname#17,author_lname#18,pages#19L,released_year#20L,stock_quantity#21L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23352a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [stock_quantity#21L, CASE WHEN (stock_quantity#21L >= 50) THEN 재고 많음 WHEN (stock_quantity#21L >= 30) THEN 재고 중간 ELSE 재고 부족 END AS quantity_level#150]\n",
      "+- *(1) Scan ExistingRDD[book_id#15L,title#16,author_fname#17,author_lname#18,pages#19L,released_year#20L,stock_quantity#21L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(books_sql_1).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c35f316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_lname#18], functions=[])\n",
      "+- Exchange hashpartitioning(author_lname#18, 200), ENSURE_REQUIREMENTS, [id=#270]\n",
      "   +- *(1) HashAggregate(keys=[author_lname#18], functions=[])\n",
      "      +- *(1) Project [author_lname#18]\n",
      "         +- *(1) Scan ExistingRDD[book_id#15L,title#16,author_fname#17,author_lname#18,pages#19L,released_year#20L,stock_quantity#21L]\n",
      "\n",
      "\n",
      "+------------+\n",
      "|author_lname|\n",
      "+------------+\n",
      "|       Jones|\n",
      "|       Davis|\n",
      "|       Smith|\n",
      "|         Doe|\n",
      "|       Brown|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_2 = '''\n",
    "select distinct author_lname from books;\n",
    "'''\n",
    "spark.sql(books_sql_2).explain()\n",
    "spark.sql(books_sql_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de96ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[author_lname#18], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(author_lname#18, 200), ENSURE_REQUIREMENTS, [id=#223]\n",
      "   +- *(1) HashAggregate(keys=[author_lname#18], functions=[partial_count(1)])\n",
      "      +- *(1) Project [author_lname#18]\n",
      "         +- *(1) Scan ExistingRDD[book_id#15L,title#16,author_fname#17,author_lname#18,pages#19L,released_year#20L,stock_quantity#21L]\n",
      "\n",
      "\n",
      "+------------+--------+\n",
      "|author_lname|count(1)|\n",
      "+------------+--------+\n",
      "|       Jones|       1|\n",
      "|       Davis|       1|\n",
      "|       Smith|       1|\n",
      "|         Doe|       1|\n",
      "|       Brown|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql_3= '''\n",
    "SELECT author_lname, count(*)\n",
    "FROM books\n",
    "GROUP BY author_lname;\n",
    "'''\n",
    "spark.sql(books_sql_3).explain()\n",
    "spark.sql(books_sql_3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643657",
   "metadata": {},
   "source": [
    "# 데이터 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c5e2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books 테이블 데이터에 borrowed_by 추가\n",
    "books_data_with_user = [\n",
    "    Row(book_id=1, title=\"Book A\", author_fname=\"John\", author_lname=\"Doe\", pages=300, released_year=2005, stock_quantity=55, borrowed_by=1),\n",
    "    Row(book_id=2, title=\"Book B\", author_fname=\"Jane\", author_lname=\"Smith\", pages=250, released_year=2010, stock_quantity=40, borrowed_by=2),\n",
    "    Row(book_id=3, title=\"Book C\", author_fname=\"Emily\", author_lname=\"Jones\", pages=180, released_year=2015, stock_quantity=20, borrowed_by=3),\n",
    "    Row(book_id=4, title=\"Book D\", author_fname=\"Chris\", author_lname=\"Brown\", pages=320, released_year=2012, stock_quantity=75, borrowed_by=None),\n",
    "    Row(book_id=5, title=\"Book E\", author_fname=\"Anna\", author_lname=\"Davis\", pages=270, released_year=2008, stock_quantity=35, borrowed_by=6)\n",
    "]\n",
    "\n",
    "# DataFrame 생성\n",
    "books_df_with_user = spark.createDataFrame(books_data_with_user)\n",
    "\n",
    "# Temp View 등록\n",
    "books_df_with_user.createOrReplaceTempView(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f39faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # borrowed_by 컬럼 추가 및 데이터 입력\n",
    "# updated_books_df = books_df.withColumn(\n",
    "#     \"borrowed_by\",\n",
    "#     when(books_df.book_id == 1, 1)\n",
    "#     .when(books_df.book_id == 2, 2)\n",
    "#     .when(books_df.book_id == 3, 3)\n",
    "#     .when(books_df.book_id == 4, lit(None))\n",
    "#     .when(books_df.book_id == 5, 6)\n",
    "#     .otherwise(None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1d32296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[book_id#203L,title#204,author_fname#205,author_lname#206,pages#207L,released_year#208L,stock_quantity#209L,borrowed_by#210L]\n",
      "\n",
      "\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            20|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_sql= '''\n",
    "SELECT *\n",
    "FROM books\n",
    "'''\n",
    "spark.sql(books_sql).explain()\n",
    "spark.sql(books_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5585a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4aa9f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|            55|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|            40|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|            50|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|            75|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|            35|          6|\n",
      "+-------+------+------------+------------+-----+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# book_id = 3, stock_quantity = 50 으로 바꿈 > 전처리 과정\n",
    "\n",
    "updated_books_df = books_df_with_user.withColumn(\n",
    "    \"stock_quantity\",\n",
    "    when (books_df_with_user.book_id == 3, 50).otherwise(books_df_with_user.stock_quantity) \n",
    ")\n",
    "\n",
    "updated_books_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c86f0e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+-----------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|   stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+-----------------+-----------+\n",
      "|      1|Book A|        John|         Doe|  300|         2005|60.50000000000001|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|             44.0|          2|\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|             22.0|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|             82.5|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|             38.5|          6|\n",
      "+-------+------+------------+------------+-----+-------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stock_quantity * 10% 증가\n",
    "\n",
    "updated_books_df = books_df_with_user.withColumn(\n",
    "    \"stock_quantity\",\n",
    "    col(\"stock_quantity\") * 1.10)\n",
    "\n",
    "#뷰로 등록\n",
    "updated_books_df.createOrReplaceTempView(\"books\")\n",
    "spark.sql(\"select * from books\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70674653",
   "metadata": {},
   "source": [
    "# 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14442a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 의 저장 mode : overwrite, append, ignore, error\n",
    "\n",
    "updated_books_df.write.csv(\"data/output/sqltest_updated_books.csv\", header = True, mode = \"overwrite\")\n",
    "user_df.write.csv(\"data/output/sqltest_user_df.csv\", header = True, mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ddebaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+-----+-------------+-----------------+-----------+\n",
      "|book_id| title|author_fname|author_lname|pages|released_year|   stock_quantity|borrowed_by|\n",
      "+-------+------+------------+------------+-----+-------------+-----------------+-----------+\n",
      "|      3|Book C|       Emily|       Jones|  180|         2015|             22.0|          3|\n",
      "|      4|Book D|       Chris|       Brown|  320|         2012|             82.5|       null|\n",
      "|      5|Book E|        Anna|       Davis|  270|         2008|             38.5|          6|\n",
      "|      1|Book A|        John|         Doe|  300|         2005|60.50000000000001|          1|\n",
      "|      2|Book B|        Jane|       Smith|  250|         2010|             44.0|          2|\n",
      "+-------+------+------------+------------+-----+-------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "updated_books_df1 = spark.read.csv(\"data/output/sqltest_updated_books.csv\", header = True)\n",
    "updated_books_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "636390ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|user_id|username|address|\n",
      "+-------+--------+-------+\n",
      "|      6|       F|   서울|\n",
      "|      7|       G| 경기도|\n",
      "|      8|       H|   대구|\n",
      "|      9|       I|   부산|\n",
      "|     10|       J|   전주|\n",
      "|     11|       K|   광주|\n",
      "|      1|       A|   서울|\n",
      "|      2|       B|   대전|\n",
      "|      3|       C| 경기도|\n",
      "|      4|       D|   null|\n",
      "|      5|       E|   null|\n",
      "+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df1 = spark.read.csv(\"data/output/sqltest_user_df.csv\", header = True)\n",
    "user_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169793be",
   "metadata": {},
   "source": [
    "# 조인 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0549e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|      5|Book E|        Anna|       Davis|       F|   서울|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INNER JOIN\n",
    "join_query = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b INNER JOIN users u ON b.borrowed_by = u.user_id;\n",
    "'''\n",
    "spark.sql(join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e92f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|      4|Book D|       Chris|       Brown|    null|   null|\n",
      "|      5|Book E|        Anna|       Davis|       F|   서울|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# books LEFT JOIN users = users RIGHT books\n",
    "\n",
    "left_join_query = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b LEFT JOIN users u ON b.borrowed_by = u.user_id;\n",
    "'''\n",
    "spark.sql(left_join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "366eeb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|   null|  null|        null|        null|       G| 경기도|\n",
      "|      5|Book E|        Anna|       Davis|       F|   서울|\n",
      "|   null|  null|        null|        null|       I|   부산|\n",
      "|   null|  null|        null|        null|       E|   null|\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|   null|  null|        null|        null|       J|   전주|\n",
      "|      3|Book C|       Emily|       Jones|       C| 경기도|\n",
      "|   null|  null|        null|        null|       H|   대구|\n",
      "|   null|  null|        null|        null|       K|   광주|\n",
      "|      2|Book B|        Jane|       Smith|       B|   대전|\n",
      "|   null|  null|        null|        null|       D|   null|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자의 책 대여 목록 > 전체 사용자 > 대여한 정보가 있으면 나오면, 없으면 NULL \n",
    "# books RIGHT JOIN users =  books LEFT JOIN users\n",
    "\n",
    "right_join_query = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b RIGHT JOIN users u ON b.borrowed_by = u.user_id;\n",
    "'''\n",
    "spark.sql(right_join_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11758669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 262:================================>                    (124 + 2) / 200]\r",
      "\r",
      "[Stage 262:================================================>    (183 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+------------+--------+-------+\n",
      "|book_id| title|author_fname|author_lname|username|address|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "|      1|Book A|        John|         Doe|       A|   서울|\n",
      "|      5|Book E|        Anna|       Davis|       F|   서울|\n",
      "+-------+------+------------+------------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 특정지역 = 서울에 거주하는 사용자가 대여한 책 목록\n",
    "\n",
    "join_query2 = '''\n",
    "SELECT book_id, title, author_fname, author_lname, username, address\n",
    "FROM books b INNER JOIN users u ON b.borrowed_by = u.user_id\n",
    "WHERE u.address = '서울'\n",
    "ORDER BY b.book_id;\n",
    "'''\n",
    "spark.sql(join_query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec42bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 283:===========================>                         (105 + 3) / 200]\r",
      "\r",
      "[Stage 283:===================================>                 (134 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+\n",
      "|user_id|username|count(book_id)|\n",
      "+-------+--------+--------------+\n",
      "|      1|       A|             1|\n",
      "|      2|       B|             1|\n",
      "|      3|       C|             1|\n",
      "|      6|       F|             1|\n",
      "+-------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 283:============================================>        (169 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 사용자별로 대여한 책 수 \n",
    "join_query2 = '''\n",
    "SELECT user_id, username, count(book_id)\n",
    "FROM users u INNER JOIN books b ON b.borrowed_by = u.user_id\n",
    "GROUP BY u.user_id, u.username\n",
    "ORDER BY u.user_id;\n",
    "'''\n",
    "spark.sql(join_query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "52ffad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+\n",
      "|book_id| title|book_category|\n",
      "+-------+------+-------------+\n",
      "|      1|Book A|         Long|\n",
      "|      2|Book B|        Short|\n",
      "|      3|Book C|        Short|\n",
      "|      4|Book D|         Long|\n",
      "|      5|Book E|        Short|\n",
      "+-------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# book_category,  page > 300 long, short\n",
    "\n",
    "join_query3 = '''\n",
    "SELECT book_id, title, CASE WHEN pages>=300 THEN 'Long' ELSE 'Short' END AS book_category\n",
    "FROM books\n",
    "'''\n",
    "spark.sql(join_query3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "533093a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------------+\n",
      "|book_id| title|   stock_quantity|stock_quantity_category|\n",
      "+-------+------+-----------------+-----------------------+\n",
      "|      1|Book A|60.50000000000001|                   충분|\n",
      "|      2|Book B|             44.0|                   보통|\n",
      "|      3|Book C|             22.0|                   부족|\n",
      "|      4|Book D|             82.5|                   충분|\n",
      "|      5|Book E|             38.5|                   보통|\n",
      "+-------+------+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stock_quantity \n",
    "join_query4 = '''\n",
    "SELECT book_id, title, stock_quantity,\n",
    "    CASE \n",
    "        WHEN stock_quantity>=50 THEN '충분' \n",
    "        WHEN stock_quantity >=30 THEN '보통' \n",
    "        ELSE '부족' \n",
    "    END AS stock_quantity_category\n",
    "FROM books\n",
    "'''\n",
    "spark.sql(join_query4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c96fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb01519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_start",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
