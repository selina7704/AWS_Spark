{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c777368",
   "metadata": {},
   "source": [
    "## Apache Spark는 다양한 배포 모드 중 로컬 모드 (Local Mode)\n",
    "```\n",
    "setMaster(\"local[*]\"):\n",
    "local[*]: 사용 가능한 모든 CPU 코어를 사용.\n",
    "local[2]: CPU 코어 2개 사용.\n",
    "local: 단일 스레드 사용.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5441d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 12:50:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/06 12:50:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# 스파크 환경 설정 객체 생성\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"restaurant-review-average\")\n",
    "spark = SparkContext(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (0, \"짜장면\", \"중식\", 125),\n",
    "    (1, \"짬뽕\", \"중식\", 235),\n",
    "    (2, \"김밥\", \"분식\", 32),\n",
    "    (3, \"떡볶이\", \"분식\", 534),\n",
    "    (4, \"라멘\", \"일식\", 223),\n",
    "    (5, \"돈가스\", \"일식\", 52),\n",
    "    (6, \"우동\", \"일식\", 12),\n",
    "    (7, \"쌀국수\", \"아시안\", 312),\n",
    "    (8, \"햄버거\", \"패스트푸드\", 12),\n",
    "    (9, \"치킨\", \"패스트푸드\", 23),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6a3209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.parallelize(data)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47abc790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '짜장면', '중식', 125), (1, '짬뽕', '중식', 235), (2, '김밥', '분식', 32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a704cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:////home/lab05/src/data/restaurant_reviews.csv MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "directory = os.path.join(os.getcwd(), \"data\")\n",
    "filename = \"restaurant_reviews.csv\"\n",
    "filepath = os.path.join(directory, filename)\n",
    "lines = spark.textFile(\"file:///\"+filepath.replace(\"\\\\\", \"/\"))\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2e6288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,item,cateogry,reviews,',\n",
       " '0,짜장면,중식,125,',\n",
       " '1,짬뽕,중식,235,',\n",
       " '2,김밥,분식,32,',\n",
       " '3,떡볶이,분식,534,']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9756c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = lines.first()\n",
    "filtered_lines = lines.filter(lambda row : row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bf037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fefc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e6b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22398f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_start",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
