{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48f8ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 14:50:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/10 14:50:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-2-110.ap-northeast-3.compute.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>241210_02_SparkSQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f90bde8d6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"241210_02_SparkSQL\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c09b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 2개 합치기\n",
    "\n",
    "# emp.csv 파일 불러오기 \n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(\"data/emp.csv\")\n",
    "\n",
    "# dept.csv 파일 불러오기 \n",
    "dept_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('data/dept.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005566b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3111581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|null|    10|\n",
      "| 9292|  JACK|    CLERK|7782|1982-01-23|3200|null|    70|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec0cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deptno: integer (nullable = true)\n",
      " |-- dname: string (nullable = true)\n",
      " |-- loc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9bcb79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7113d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 파일, 데이터프레임의 VIEW 만들기 \n",
    "df.createOrReplaceTempView(\"emp_df_view\")\n",
    "dept_df.createOrReplaceTempView(\"dept_df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "883ef28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|deptno|     dname|     loc|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|    30|     SALES| CHICAGO|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|    30|     SALES| CHICAGO|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|null|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|    30|     SALES| CHICAGO|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|    30|     SALES| CHICAGO|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|null|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|null|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|    30|     SALES| CHICAGO|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|null|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|    30|     SALES| CHICAGO|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|null|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|null|    10|    10|ACCOUNTING|NEW YORK|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 뷰에서 조인 \n",
    "\n",
    "join_query = '''\n",
    "SELECT *\n",
    "FROM emp_df_view JOIN dept_df_view ON emp_df_view.deptno = dept_df_view.deptno\n",
    "'''\n",
    "\n",
    "viewEmpDept = spark.sql(join_query)\n",
    "viewEmpDept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c45ad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|deptno|     dname|     loc|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|null|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|null|    10|    10|ACCOUNTING|NEW YORK|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 조인된 데이터프레임 테이블 VIEW만들기\n",
    "viewEmpDept.createOrReplaceTempView(\"viewEmpDept\")\n",
    "\n",
    "# LOC 이 NEW YORK 인 곳 필터링 \n",
    "v_query = '''\n",
    "select *\n",
    "from viewEmpDept\n",
    "where loc = \"NEW YORK\"\n",
    "'''\n",
    "\n",
    "#case1\n",
    "spark.sql(v_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620029bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_query = '''\n",
    "select *\n",
    "from viewEmpDept\n",
    "where loc = \"NEW YORK\"\n",
    "'''\n",
    "spark.sql(v_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193acb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|null|    10|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subquery\n",
    "\n",
    "query2 = '''\n",
    "select *\n",
    "from emp_df_view\n",
    "where deptno = (\n",
    "    select deptno\n",
    "    from dept_df_view\n",
    "    where loc = 'NEW YORK'\n",
    ")\n",
    "'''\n",
    "\n",
    "#case 2 \n",
    "spark.sql(query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62c18703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [deptno#23], [deptno#111], Inner, BuildRight, false\n",
      ":- *(2) Filter isnotnull(deptno#23)\n",
      ":  +- FileScan csv [empno#16,ename#17,job#18,mgr#19,hiredate#20,sal#21,comm#22,deptno#23] Batched: false, DataFilters: [isnotnull(deptno#23)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab05/src/data/emp.csv], PartitionFilters: [], PushedFilters: [IsNotNull(deptno)], ReadSchema: struct<empno:int,ename:string,job:string,mgr:int,hiredate:string,sal:int,comm:int,deptno:int>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#315]\n",
      "   +- *(1) Filter ((isnotnull(loc#113) AND (loc#113 = NEW YORK)) AND isnotnull(deptno#111))\n",
      "      +- FileScan csv [deptno#111,dname#112,loc#113] Batched: false, DataFilters: [isnotnull(loc#113), (loc#113 = NEW YORK), isnotnull(deptno#111)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab05/src/data/dept.csv], PartitionFilters: [], PushedFilters: [IsNotNull(loc), EqualTo(loc,NEW YORK), IsNotNull(deptno)], ReadSchema: struct<deptno:int,dname:string,loc:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#case1의 실행 계획 - join\n",
    "spark.sql(v_query).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a23bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(deptno#23) AND (deptno#23 = Subquery scalar-subquery#478, [id=#334]))\n",
      ":  +- Subquery scalar-subquery#478, [id=#334]\n",
      ":     +- *(1) Project [deptno#111]\n",
      ":        +- *(1) Filter (isnotnull(loc#113) AND (loc#113 = NEW YORK))\n",
      ":           +- FileScan csv [deptno#111,loc#113] Batched: false, DataFilters: [isnotnull(loc#113), (loc#113 = NEW YORK)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab05/src/data/dept.csv], PartitionFilters: [], PushedFilters: [IsNotNull(loc), EqualTo(loc,NEW YORK)], ReadSchema: struct<deptno:int,loc:string>\n",
      "+- FileScan csv [empno#16,ename#17,job#18,mgr#19,hiredate#20,sal#21,comm#22,deptno#23] Batched: false, DataFilters: [isnotnull(deptno#23)], Format: CSV, Location: InMemoryFileIndex[file:/home/lab05/src/data/emp.csv], PartitionFilters: [], PushedFilters: [IsNotNull(deptno)], ReadSchema: struct<empno:int,ename:string,job:string,mgr:int,hiredate:string,sal:int,comm:int,deptno:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#case 2 \n",
    "spark.sql(query2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a105b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_start",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
